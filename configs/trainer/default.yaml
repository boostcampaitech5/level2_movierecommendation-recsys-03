# epoch for training
epoch: 200

# epoch
pre_epochs: 300

# learning rate
lr: 0.001

# weight_decay for learning rate
weight_decay: 0.0

# todo: fill
log_freq: 1

# weight of aap loss
aap_weight: 0.2

# weight of min loss
mip_weight: 1.0

# weight of map loss
map_weight: 1.0

# weight of sp loss
sp_weight: 0.5

# betas of adam optimizer 
adam_beta1: 0.9
adam_beta2: 0.999